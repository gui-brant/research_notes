Notes based on: http://incompleteideas.net/book/RLbook2020.pdf

1.1 RL: trial-and-error search and delayed reward are the two most important features of RL.

- The basic idea is simply to capture the most important aspects of the issues faced by the learning agent over time when trying to achieve something through an environment. 
	The agent senses a state of the environment and takes actions to change this state to achieve some goal.
- Differently than supervised learning, this one learns by experience. There are no labels. There are no "correct" answer. There is trial, error, and learning. 
	Unsupervised learning also doesn't fit RL. It tries to find patterns in unlabeled data to group them together. The core difference is that RL attempts to maximize reward signals instead of looking for hidden structure. 
	To be clear, supervised and unsupervised learning are both useful, but not necessary to the goals of RL. 
- A very core point to RL is the trade-off between exploration and exploitation. It collect reward for exploiting what it knows, but also explore new "ideas" to potentially push for improved rewards.
	This issue is still unresolved by mathematicians, but it is certainly been extensively studied. 
	RL is not limited to focusing on subproblems. It considers the whole of the environment to tackle decision-making.
	RL also considers planning. It plans and acts, but must also adapt while acting to ensure optimal results. As I said before, it's strategy. Yes, you plan, but the framework must also adapt mid-play as to not be too stuck on the plan.
	You can use supervised learning as a "planner." It tackles subproblems that the agent might face. 
	RL framework are, in fact, soooo general that you can have RL in RL systems. You can use a "complete, interactive, goal-seeking agent" to monitor charge levels of robots. Then, this RL's environment is the robot, who has another 	environment himself.
- This is the one framework that has gotten most inspiration from actual humans and animals and has, in fact, assisted in research towards things like psychology and neuroscience.- 

1.2 Examples: the most trivial of things in your life rely on agent-environment interaction. It is by trying and failing that you get to figure out how to optimize almost any task in your life, be that figuring out complex equation or making lunch. 

1.3 Elements of RL: other than agent and environment, we have POLICY, VALUE FUNCTION, and the optional MODEL.
	- POLICY: defines the learning agent's way of behaving at a given time. It's the core of any agent. It alone should be able to determine behavior. Generally, it may be stochastic, meaning finding probabilities for each action.
	- REWARD SIGNAL: stochastic functions of the state of the environment with the actions taken. It defines the goal of an RL problem: to maximize the reward generated by the policies upon the environment's state. In other words, it 	changes the policy to maximize reward for every action, working closely with the VALUE FUNCTION. 
	- VALUE FUNCTION: it defines what is good in the LONG-RUN. The agent will gain immediate, intrinsic rewards after each action, but it may prefer a lower reward for higher value, if it judges that, by taking this low-reward action, 	its future state will be of greater reward than the immediate one. 
		Rewards are primary, while value is secondary, as value predicts reward. Just like any person ever, rewards are the basic building block, but we are most concerned with value because that is, at the end of the day, what 		tends to maximize reward. This also implies that value is the most complicated bit, as it must be deduced or inferred or evaluated based on the lifetime of the agent. A good RL method is a good method to estimate value. 
	- MODELS: this one is optional. These guys are used for planning. They allow the model to consider future outcomes before they are experienced. 

1.4 Limitations and Scope: the book does not go in detail into "how to design state signals" (stuff like gold metrics). 
 	- The book also focuses a lot on optimizing value, which is likely what we're interested on. However, it makes it quite clear that this is not the only way of doing effective RL. One may use, for instance, evolutionary methods. 	They are simply a bunch of policies that try different things. The one with the best immediate reward outcome gets carried over to the next "generation" of the model (just like biological evolution). This is pretty effective in 	some cases, like genetic programming. 
	To quote them, "if the space of policies is sufficiently small, or can be structures so that good policies are common or easy to find - or if a lot of time is available for the search - then 	evolutionary methods can be effective. 	In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment."
	In short, the authors think evolutionary programming is pretty weak, hence they don't cover it. I seem to agree instinctively to this.

1.5 An Extended Example: Tic-Tac-Toe
	- The logical procedure of RL: 
	Set up table of numbers per state of the game --> they are estimates of probability of winning from a certain state --> so each state is a table, and each table/state there are several different changes of winning, recorded as the 	numbers/prob estimates --> these probabilities or numbers are the value (as they evaluate the chance of victory by the move per state)... each table is the learned value function --> we set every number or estimate, other than 	three rowed Xs and Os (that would be 1 for win and 0 for loss), as the initial guess of 0.5 --> many games are played --> in each game, we examine the different states resulting from each possible move, and look up their current 	value/estimate on the table/value function --> most times, we are "greedy," meaning we select the state with highest value, but occasionally also select a different one (called exploratory moves) --> the result to every "greedy" 	move causes a chain of events to modify the "greedy" value/estimate (this is explained in more detailed in diagram 1.1).
	This process is called temporal-difference since it learns between estimates at two successive times. It's pretty good at this particular task.
	- The frequency of win gives an unbiased esteimate of the probability of winning with that policy, hence it's used to select the next policy. 
	- Value functions allow us to take into account every play. If not for it, then the policy would credit the win and only the win, regardless of the many situations that occured. In others words, if you get a "lucky" play and 	win, the model never learns what is a correct play. 
	- Backgammon has 10^20 states, unlike tic-tac-toe, but combining this model with neural networks, a program made in the mid-90s was able to beat the best player in the world. NNs allows the program to generalize from its 	experiences, without needing to learn every state. It searches similar states it has saved on the NN, allowing it to "remember." 

	- Exercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? 	Would it learn a di↵erent policy for selecting moves? --> They would likely be stuck in a loop. Eventually, they would come to the conclusion that whoever starts wins, and the games would start looking essentially the same. In 	other words, they would likely not learn other policies. Even if we don't assume that they figure out the "start=win" rule, they would still opearte very similarly every time.
	- Exercise 1.2: Symmetries Many tic-tac-toe positions appear di↵erent but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this 	change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same 	value? --> This essentially simplifies the number of possible plays or sets. We can likely code this into the program, maybe through NN, which would streamline the learning process by decreasing possible states to be learned. 	It seems to me like taking advantage of symmetrics is just benefitial in general, so, yes, we should. Yes, in other words, they should have the same value. 
	- Exercise 1.3:  Greedy Play Suppose the reinforcement learning player was greedy,  that is, it always played the move that brought it to the position that it rated the best.  Might it learn to play better, or worse, than a nongreedy player? What problems 	might occur? --> The model loses the opportunity for potential learning streategies by being super-greedy. There is a chance, specially if model-free, that a certain superior value to the one currently considered "the best" is 	overlooked if this is the case, as per cause-and-effect. The model becomes, in other words, predictable and uncapable of higher adaptation.
	- Exercise 1.4: Learning from Exploration Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would 	converge to a di↵erent set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities 	might be better to learn? Which would result in more wins? --> The odd of winning and the odd of losing. The set of chances of "outplay." In other words, the chance that you will make a move that will cause your opponent to lose and 	you to win. That does not mean that you're looking for "move=win," in fact you're looking for "move=value=more chances of beating this opponent." Likely, the latter since it allows for adaptation. 
	- Exercise 1.5: Other Improvements Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed? --> I think the RL model can benefit a lot from a NN model. Tic-	tac-toe, like many strategy games, has a set pattern to some situations that will, almost always (in this case, it is always), lead to the optimal result, so it should benefit a lot from a simple NN.
